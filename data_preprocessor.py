"""
Data Preprocessing Module for Hospital Readmission Prediction Pipeline
"""
import pandas as pd

import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import logging
from typing import Dict, Tuple, List, Optional
import warnings

from pipeline_config import ICD9_CATEGORIES, FEATURE_CATEGORIES

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ç±»ï¼Œè´Ÿè´£ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®æ¸…æ´—"""
    
    def __init__(self):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_engineering_applied = False
        
    def icd9_to_nine_category(self, code: str) -> str:
        """
        å°†ICD-9ä»£ç åˆ†ç±»ä¸º9ä¸ªä¸»è¦ç±»åˆ«
        
        Args:
            code: ICD-9ä»£ç 
            
        Returns:
            åˆ†ç±»ç»“æœ
        """
        try:
            code = str(code).strip()
            if code.startswith('E') or code.startswith('V'):
                return 'other'
            
            num = float(code)
            
            for category, (start, end) in ICD9_CATEGORIES.items():
                if start <= num <= end:
                    return category
            
            return 'other'
        except:
            return 'other'
    
    def create_age_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºå¹´é¾„ç›¸å…³ç‰¹å¾
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            æ·»åŠ å¹´é¾„ç‰¹å¾åçš„æ•°æ®æ¡†
        """
        logger.info("Creating age-related features...")
        
        # åˆ›å»ºå¹´é¾„ä¸­ç‚¹
        df['age_midpoint'] = df['age'].apply(lambda x: 
            int(x.replace('[', '').replace(')', '').split('-')[0]) + 
            int(x.replace('[', '').replace(')', '').split('-')[1]) // 2
        )
        
        # åˆ›å»ºå¹´é¾„ç»„
        df['age_group'] = df['age'].apply(lambda x: 
            x.replace('[', '').replace(')', '')
        )
        
        return df
    
    def create_diagnosis_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºè¯Šæ–­ç›¸å…³ç‰¹å¾
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            æ·»åŠ è¯Šæ–­ç‰¹å¾åçš„æ•°æ®æ¡†
        """
        logger.info("Creating diagnosis-related features...")
        
        # ä¸ºæ¯ä¸ªè¯Šæ–­åˆ›å»ºåˆ†ç±»ç‰¹å¾
        for col in ['diag_1', 'diag_2', 'diag_3']:
            df[f"{col}_category"] = df[col].apply(self.icd9_to_nine_category)
        
        return df
    
    def create_comorbidity_feature(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºåˆå¹¶ç—‡ç‰¹å¾
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            æ·»åŠ åˆå¹¶ç—‡ç‰¹å¾åçš„æ•°æ®æ¡†
        """
        logger.info("Creating comorbidity feature...")
        
        # è®¡ç®—åˆå¹¶ç—‡æ•°é‡ï¼ˆåŸºäºè¯Šæ–­æ•°é‡ï¼‰
        df['comorbidity'] = df['number_diagnoses'].apply(lambda x: 
            0 if x <= 1 else (1 if x <= 3 else 2)
        )
        
        return df
    
    def create_encounter_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºå°±è¯Šç›¸å…³ç‰¹å¾
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            æ·»åŠ å°±è¯Šç‰¹å¾åçš„æ•°æ®æ¡†
        """
        logger.info("Creating encounter-related features...")
        
        # åˆ›å»ºå°±è¯Šç´¢å¼•
        df['encounter_index'] = df.groupby('patient_nbr').cumcount() + 1
        
        # åˆ›å»ºæ»šåŠ¨å¹³å‡ç‰¹å¾
        df['rolling_avg'] = df.groupby('patient_nbr')['time_in_hospital'].transform(
            lambda x: x.expanding().mean()
        )
        
        return df
    
    def handle_missing_values(self, df: pd.DataFrame, drop_missing_threshold: float = 50.0) -> pd.DataFrame:
        """
        æ ¹æ®åŸå§‹notebookçš„æ ‡å‡†å¤„ç†ç¼ºå¤±å€¼ã€‚

        - é¦–å…ˆï¼Œå°†æ‰€æœ‰'?'æ›¿æ¢ä¸ºNaNã€‚
        - åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡`drop_missing_threshold`%çš„åˆ—ã€‚
        - å°†æŒ‡å®šåˆ—çš„å‰©ä½™ç¼ºå¤±å€¼å¡«å……ä¸º'Unknown'ã€‚

        Args:
            df: è¾“å…¥æ•°æ®æ¡†ã€‚
            drop_missing_threshold: åˆ é™¤åˆ—çš„ç¼ºå¤±ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚

        Returns:
            å¤„ç†ç¼ºå¤±å€¼åçš„æ•°æ®æ¡†ã€‚
        """
        logger.info("Handling missing values based on notebook's standard...")

        # 1. å°†'?'æ›¿æ¢ä¸ºNaN
        df = df.replace('?', np.nan)
        logger.info("Replaced '?' with NaN.")

        # 2. è®¡ç®—å¹¶åˆ é™¤é«˜ç¼ºå¤±ç‡çš„åˆ—
        missing_percentage = (df.isnull().sum() / len(df)) * 100
        cols_to_drop = missing_percentage[missing_percentage > drop_missing_threshold].index
        if len(cols_to_drop) > 0:
            df = df.drop(columns=cols_to_drop)
            logger.info(f"ğŸ”´ Dropped columns with >{drop_missing_threshold}% missing: {list(cols_to_drop)}")
        else:
            logger.info(f"âœ… No columns with >{drop_missing_threshold}% missing values to drop.")

        # 3. å¡«å……å‰©ä½™çš„ç¼ºå¤±å€¼
        cols_to_fill = [
            'medical_specialty', 'payer_code', 'race',
            'diag_1', 'diag_2', 'diag_3',
            'admission_type_desc', 'discharge_disposition_desc', 'admission_source_desc'
        ]
        
        # ç­›é€‰å‡ºæ•°æ®æ¡†ä¸­å®é™…å­˜åœ¨çš„ã€éœ€è¦å¡«å……çš„åˆ—
        existing_cols_to_fill = [col for col in cols_to_fill if col in df.columns and df[col].isnull().any()]

        if existing_cols_to_fill:
            logger.info(f"ğŸŸ¡ Filling specified columns with 'Unknown': {existing_cols_to_fill}")
            df[existing_cols_to_fill] = df[existing_cols_to_fill].fillna('Unknown')
        else:
            logger.info("âœ… No missing values found in the specified columns to fill.")

        # æœ€ç»ˆæ£€æŸ¥
        final_missing = df.isnull().sum().sum()
        if final_missing == 0:
            logger.info("âœ… All missing values have been handled successfully.")
        else:
            logger.warning(f"âš ï¸ Still have {final_missing} missing values after processing.")
            remaining_cols = df.isnull().sum()
            logger.warning(f"Columns with remaining NaNs:\n{remaining_cols[remaining_cols > 0]}")

        return df
    
    def apply_feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åº”ç”¨ç‰¹å¾å·¥ç¨‹
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®æ¡†
        """
        logger.info("Applying feature engineering...")
        
        # 1. ä¿ç•™æ¯ä¸ªæ‚£è€…çš„ç¬¬ä¸€æ¬¡å…¥é™¢è®°å½•
        logger.info(f"Original number of encounters: {len(df)}")
        df_sorted = df.sort_values(by='encounter_id')
        df = df_sorted.drop_duplicates(subset='patient_nbr', keep='first')
        logger.info(f"Encounters after keeping first admission: {len(df)}")
        
        # 2. ç§»é™¤å»ä¸–æˆ–å»ä¸´ç»ˆå…³æ€€çš„æ‚£è€…
        hospice_or_death_ids = [11, 13, 14, 19, 20, 21]
        records_before_filter = len(df)
        df = df[~df['discharge_disposition_id'].isin(hospice_or_death_ids)]
        logger.info(f"Removed {records_before_filter - len(df)} records for hospice/death dispositions.")
        logger.info(f"Encounters after removing hospice/death: {len(df)}")

        # 3. å¤„ç†ç¼ºå¤±å€¼
        df = self.handle_missing_values(df)
        
        # 4. åˆ›å»ºå„ç§ç‰¹å¾
        df = self.create_age_features(df)
        df = self.create_diagnosis_features(df)
        df = self.create_comorbidity_feature(df)
        df = self.create_encounter_features(df)
        
        self.feature_engineering_applied = True
        logger.info("Feature engineering completed")
        
        return df
    
    def prepare_target_variable(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å‡†å¤‡ç›®æ ‡å˜é‡
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            å¤„ç†ç›®æ ‡å˜é‡åçš„æ•°æ®æ¡†
        """
        logger.info("Preparing target variable...")
        
        # å°†ç›®æ ‡å˜é‡è½¬æ¢ä¸ºäºŒè¿›åˆ¶ï¼ˆ0: æœªå†å…¥é™¢, 1: å†å…¥é™¢ï¼‰
        df['readmitted_binary'] = (df['readmitted'] == '<30').astype(int)
        
        return df
    
    def split_data(self, df: pd.DataFrame, target_col: str = 'readmitted_binary',
                   test_size: float = 0.2, val_size: float = 0.2, 
                   random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
        """
        åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶åœ¨åˆ†å‰²å‰åˆ é™¤æ— ç”¨çš„IDåˆ—ã€‚

        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            target_col: ç›®æ ‡å˜é‡åˆ—å
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            val_size: éªŒè¯é›†æ¯”ä¾‹
            random_state: éšæœºç§å­

        Returns:
            è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„ç‰¹å¾å’Œç›®æ ‡å˜é‡
        """
        logger.info("Splitting data into train/validation/test sets...")

        # 1. å®šä¹‰å¹¶åˆ é™¤å¯¹æ¨¡å‹è®­ç»ƒæ— ç”¨çš„IDåˆ—
        cols_to_drop = [
            'encounter_id', 'patient_nbr', 
            'admission_type_id', 'discharge_disposition_id', 'admission_source_id',
            'readmitted'  # åŸå§‹ç›®æ ‡å˜é‡ï¼Œå·²è¢«äºŒè¿›åˆ¶ç‰ˆæœ¬æ›¿ä»£
        ]
        
        # ç­›é€‰å‡ºå®é™…å­˜åœ¨äºDataFrameä¸­çš„åˆ—è¿›è¡Œåˆ é™¤
        existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]
        if existing_cols_to_drop:
            logger.info(f"Dropping unused ID and target columns before splitting: {existing_cols_to_drop}")
            X = df.drop(columns=existing_cols_to_drop + [target_col])
        else:
            X = df.drop(columns=[target_col])

        y = df[target_col]

        # 2. é¦–å…ˆåˆ†å‰²å‡ºæµ‹è¯•é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        # 3. ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size, random_state=random_state, stratify=y_temp
        )

        logger.info(f"Data split - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def encode_categorical_features(self, X_train: pd.DataFrame, X_val: pd.DataFrame, 
                                  X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        ç¼–ç åˆ†ç±»ç‰¹å¾
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_val: éªŒè¯é›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            
        Returns:
            ç¼–ç åçš„ç‰¹å¾é›†
        """
        logger.info("Encoding categorical features...")
        
        categorical_features = X_train.select_dtypes(include=['object']).columns
        
        for col in categorical_features:
            le = LabelEncoder()
            X_train[col] = le.fit_transform(X_train[col])
            X_val[col] = X_val[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)
            X_test[col] = X_test[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)
            self.label_encoders[col] = le
        
        return X_train, X_val, X_test
    
    def scale_numerical_features(self, X_train: pd.DataFrame, X_val: pd.DataFrame, 
                                X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_val: éªŒè¯é›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            
        Returns:
            æ ‡å‡†åŒ–åçš„ç‰¹å¾é›†
        """
        logger.info("Scaling numerical features...")
        
        numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns
        
        # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
        self.scaler.fit(X_train[numerical_features])
        
        # è½¬æ¢æ‰€æœ‰æ•°æ®é›†
        X_train[numerical_features] = self.scaler.transform(X_train[numerical_features])
        X_val[numerical_features] = self.scaler.transform(X_val[numerical_features])
        X_test[numerical_features] = self.scaler.transform(X_test[numerical_features])
        
        return X_train, X_val, X_test
    
    def apply_smote(self, X_train: pd.DataFrame, y_train: pd.Series, 
                   random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åº”ç”¨SMOTEæ¥å¹³è¡¡è®­ç»ƒæ•°æ®é›†ã€‚

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            random_state: éšæœºç§å­

        Returns:
            å¹³è¡¡åçš„è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾
        """
        logger.info("Applying SMOTE for class balancing...")
        
        smote = SMOTE(random_state=random_state)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        logger.info(f"Before SMOTE - Class distribution: {y_train.value_counts().to_dict()}")
        logger.info(f"After SMOTE - Class distribution: {y_train_balanced.value_counts().to_dict()}")
        
        return X_train_balanced, y_train_balanced
    
    def get_feature_categories(self) -> Dict[str, List[str]]:
        """
        è·å–ç‰¹å¾åˆ†ç±»ä¿¡æ¯
        
        Returns:
            ç‰¹å¾åˆ†ç±»å­—å…¸
        """
        return FEATURE_CATEGORIES
    
    def save_preprocessed_data(self, X_train: pd.DataFrame, X_val: pd.DataFrame, 
                             X_test: pd.DataFrame, y_train: pd.Series, y_val: pd.Series, 
                             y_test: pd.Series, output_dir: str = 'outputs') -> None:
        """
        ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            X_val: éªŒè¯é›†ç‰¹å¾
            X_test: æµ‹è¯•é›†ç‰¹å¾
            y_train: è®­ç»ƒé›†ç›®æ ‡å˜é‡
            y_val: éªŒè¯é›†ç›®æ ‡å˜é‡
            y_test: æµ‹è¯•é›†ç›®æ ‡å˜é‡
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info("Saving preprocessed data...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        X_train.to_csv(f"{output_dir}/X_train.csv", index=False)
        X_val.to_csv(f"{output_dir}/X_val.csv", index=False)
        X_test.to_csv(f"{output_dir}/X_test.csv", index=False)
        y_train.to_csv(f"{output_dir}/y_train.csv", index=False)
        y_val.to_csv(f"{output_dir}/y_val.csv", index=False)
        y_test.to_csv(f"{output_dir}/y_test.csv", index=False)
        
        logger.info(f"Preprocessed data saved to {output_dir}")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ•°æ®é¢„å¤„ç†åŠŸèƒ½"""
    from data_loader import DataLoader
    
    # åŠ è½½æ•°æ®
    loader = DataLoader()
    df = loader.merge_data()
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = DataPreprocessor()
    
    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    df = preprocessor.apply_feature_engineering(df)
    
    # å‡†å¤‡ç›®æ ‡å˜é‡
    df = preprocessor.prepare_target_variable(df)
    
    # åˆ†å‰²æ•°æ®
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(df)
    
    # ç¼–ç åˆ†ç±»ç‰¹å¾
    X_train, X_val, X_test = preprocessor.encode_categorical_features(X_train, X_val, X_test)
    
    # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
    X_train, X_val, X_test = preprocessor.scale_numerical_features(X_train, X_val, X_test)
    
    # åº”ç”¨SMOTE
    X_train_balanced, y_train_balanced = preprocessor.apply_smote(X_train, y_train)
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
    preprocessor.save_preprocessed_data(X_train_balanced, X_val, X_test, 
                                      y_train_balanced, y_val, y_test)
    
    print("Data preprocessing completed successfully!")
    return X_train_balanced, X_val, X_test, y_train_balanced, y_val, y_test

if __name__ == "__main__":
    main() 