"""
Model Training Module for Hospital Readmission Prediction Pipeline
"""
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                           roc_auc_score, confusion_matrix, classification_report)
import logging
from typing import Dict, List, Tuple, Any
import joblib
from pathlib import Path
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

from pipeline_config import MODEL_CONFIG, MODELS

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ç±»ï¼Œè´Ÿè´£è®­ç»ƒå’Œè¯„ä¼°å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, random_state: int = 42):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            random_state: éšæœºç§å­
        """
        self.random_state = random_state
        self.models = {}
        self.trained_models = {}
        self.cv_results = {}
        self.test_results = {}
        
    def get_models(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        
        Returns:
            æ¨¡å‹å­—å…¸
        """
        return {
            'LogisticRegression': LogisticRegression(solver='liblinear', random_state=self.random_state),
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=self.random_state),
            'XGBoost': XGBClassifier(eval_metric='logloss', random_state=self.random_state)
        }
    
    def evaluate_model_with_cv(self, model: Any, X: pd.DataFrame, y: pd.Series, 
                              cv_folds: int = 5) -> Tuple[float, float]:
        """
        ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹
        
        Args:
            model: æœºå™¨å­¦ä¹ æ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            
        Returns:
            AUCå’ŒF1åˆ†æ•°çš„å¹³å‡å€¼
        """
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        auc_scores = []
        f1_scores = []
        
        for train_idx, test_idx in skf.split(X, y):
            X_train_cv, X_test_cv = X.iloc[train_idx], X.iloc[test_idx]
            y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]
            
            model.fit(X_train_cv, y_train_cv)
            y_pred = model.predict(X_test_cv)
            y_prob = model.predict_proba(X_test_cv)[:, 1]
            
            auc_scores.append(roc_auc_score(y_test_cv, y_prob))
            f1_scores.append(f1_score(y_test_cv, y_pred))
        
        return np.mean(auc_scores), np.mean(f1_scores)
    
    def train_single_model(self, model_name: str, X_train: pd.DataFrame, y_train: pd.Series) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹ã€‚
        """
        logger.info(f"Training {model_name}...")
        
        models = self.get_models()
        if model_name not in models:
            raise ValueError(f"Unknown model: {model_name}. Available models: {list(models.keys())}")
        
        model = models[model_name]
        
        # ç›´æ¥åœ¨ä¼ å…¥çš„æ•°æ®ä¸Šè®­ç»ƒ
        model.fit(X_train, y_train)
        
        # åœ¨åŒæ ·çš„æ•°æ®ä¸Šè¿›è¡Œäº¤å‰éªŒè¯
        auc_cv, f1_cv = self.evaluate_model_with_cv(model, X_train, y_train, 
                                                   cv_folds=MODEL_CONFIG['cv_folds'])
        
        results = {
            'model_name': model_name,
            'cv_auc': auc_cv,
            'cv_f1': f1_cv
        }
        
        logger.info(f"{model_name} training completed - CV AUC: {auc_cv:.3f}, CV F1: {f1_cv:.3f}")
        return results
    
    def train_all_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> pd.DataFrame:
        """
        è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†ç›®æ ‡å˜é‡
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒç»“æœDataFrame
        """
        logger.info("Training all models...")
        
        models = self.get_models()
        results = []
        
        for model_name in models.keys():
            try:
                result = self.train_single_model(model_name, X_train, y_train)
                results.append(result)
            except Exception as e:
                logger.error(f"Error training {model_name}: {e}")
                results.append({
                    'model_name': model_name,
                    'cv_auc': 0.0,
                    'cv_f1': 0.0
                })
        
        results_df = pd.DataFrame(results)
        logger.info("All models training completed")
        
        return results_df
    
    def train_models_for_feature_sets(self, 
                                     feature_sets: Dict[int, Dict[str, List[str]]],
                                     X_train: pd.DataFrame, 
                                     y_train: pd.Series) -> pd.DataFrame:
        """
        ä¸ºå¤šä¸ªç‰¹å¾é›†è®­ç»ƒæ‰€æœ‰æ¨¡å‹

        Args:
            feature_sets: ç‰¹å¾é›†å­—å…¸, æ ¼å¼ä¸º {top_n: {method: [features...]}}
            X_train: å®Œæ•´çš„è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†ç›®æ ‡å˜é‡

        Returns:
            ä¸€ä¸ªåŒ…å«æ‰€æœ‰åœºæ™¯ä¸‹æ¨¡å‹æ€§èƒ½çš„DataFrame
        """
        logger.info("Starting model training for multiple feature sets...")
        all_results = []

        for top_n, methods in feature_sets.items():
            for method, features in methods.items():
                if not features:
                    logger.warning(f"Skipping training for top_n={top_n}, method={method} due to empty feature list.")
                    continue

                logger.info(f"--- Training for: top_n={top_n}, method={method} ---")
                
                # é€‰å–å½“å‰åœºæ™¯çš„ç‰¹å¾å­é›†
                X_train_subset = X_train[features]
                
                # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
                scenario_results_df = self.train_all_models(X_train_subset, y_train)
                
                # æ·»åŠ åœºæ™¯ä¿¡æ¯
                scenario_results_df['top_n'] = top_n
                scenario_results_df['feature_method'] = method
                
                all_results.append(scenario_results_df)

        if not all_results:
            logger.error("No models were trained. Please check feature sets.")
            return pd.DataFrame()

        final_results_df = pd.concat(all_results, ignore_index=True)
        logger.info("Completed model training for all feature sets.")
        return final_results_df

    def display_training_results(self, results_df: pd.DataFrame, metric='cv_auc') -> None:
        """
        ä»¥è¡¨æ ¼å’Œé€è§†è¡¨çš„å½¢å¼å±•ç¤ºå¤šåœºæ™¯è®­ç»ƒç»“æœ

        Args:
            results_df: æ¥è‡ª train_models_for_feature_sets çš„ç»“æœ
            metric: ç”¨äºåœ¨é€è§†è¡¨ä¸­å±•ç¤ºçš„æ ¸å¿ƒæŒ‡æ ‡
        """
        try:
            from IPython.display import display
        except ImportError:
            display = print
            
        print("\nğŸ“Š è¯¦ç»†æ¨¡å‹è®­ç»ƒç»“æœ:")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None):
            display(results_df)

        print(f"\nğŸ“ˆ æŒ‰'{metric}'æŒ‡æ ‡è¡¨ç°çš„æ€§èƒ½æ€»ç»“ (é€è§†è¡¨):")
        
        # åˆ›å»ºé€è§†è¡¨
        pivot_table = results_df.pivot_table(
            index=['top_n', 'feature_method'], 
            columns='model_name', 
            values=metric
        )
        
        # é«˜äº®æ¯è¡Œçš„æœ€å¤§å€¼
        display(pivot_table.style.highlight_max(axis=1, color='lightgreen'))

    def plot_training_results(self, results_df: pd.DataFrame, metric='cv_auc', save_path: str = None) -> None:
        """
        å¯è§†åŒ–å¤šåœºæ™¯è®­ç»ƒç»“æœ

        Args:
            results_df: æ¥è‡ª train_models_for_feature_sets çš„ç»“æœ
            metric: ç”¨äºå¯è§†åŒ–çš„æ ¸å¿ƒæŒ‡æ ‡
            save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
        """
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œä»¥é˜²ä¸‡ä¸€
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
        except ImportError:
            logger.warning("matplotlib or seaborn not found. Skipping plotting.")
            return

        print(f"\nğŸ¨ ç”ŸæˆåŸºäº'{metric}'æŒ‡æ ‡çš„æ€§èƒ½å¯è§†åŒ–å›¾è¡¨:")

        # ä½¿ç”¨catplotå¯ä»¥è½»æ¾åˆ›å»ºæŒ‰top_nåˆ†ç»„çš„æ¡å½¢å›¾
        g = sns.catplot(
            data=results_df,
            x='model_name',
            y=metric,
            hue='feature_method',
            col='top_n',
            kind='bar',
            height=6,
            aspect=0.9,
            palette='viridis',
            legend=False
        )

        # è°ƒæ•´å›¾è¡¨ç»†èŠ‚
        g.fig.suptitle(f'å„æ¨¡å‹åœ¨ä¸åŒTop Nå’Œç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸‹çš„æ€§èƒ½ ({metric})', y=1.03, size=16)
        g.set_axis_labels("æœºå™¨å­¦ä¹ æ¨¡å‹", f"æ€§èƒ½å¾—åˆ† ({metric})")
        g.set_titles("Top N = {col_name}")
        g.despine(left=True)

        # ä¸ºæ¯ä¸ªå­å›¾æ·»åŠ æ•°å€¼æ ‡ç­¾
        for ax in g.axes.flat:
            for p in ax.patches:
                ax.annotate(f'{p.get_height():.3f}',
                            (p.get_x() + p.get_width() / 2., p.get_height()),
                            ha='center', va='center',
                            xytext=(0, 9),
                            textcoords='offset points',
                            fontsize=9)
            ax.tick_params(axis='x', rotation=30)

        # æ·»åŠ å›¾ä¾‹
        plt.legend(title='ç‰¹å¾é€‰æ‹©æ–¹æ³•', bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout(rect=[0, 0, 0.9, 0.96])

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Training results plot saved to: {save_path}")

        plt.show()

    def plot_performance_vs_top_n(self, results_df: pd.DataFrame, save_path: str = None) -> None:
        """
        ç»˜åˆ¶æ¨¡å‹æ€§èƒ½éš top_n å˜åŒ–çš„æ›²çº¿å›¾ã€‚

        Args:
            results_df: æ¥è‡ª train_models_for_feature_sets çš„ç»“æœã€‚
            save_path: å¯é€‰çš„å›¾ç‰‡ä¿å­˜è·¯å¾„ã€‚
        """
        if not _PLOTTING_ENABLED: return
        print("\nğŸ“ˆ ç”Ÿæˆæ¨¡å‹æ€§èƒ½éšç‰¹å¾æ•°é‡å˜åŒ–çš„è¶‹åŠ¿å›¾:")

        # ä¸ºä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼ˆAUC å’Œ F1ï¼‰åˆ†åˆ«ç»˜å›¾
        for metric in ['cv_auc', 'cv_f1']:
            plt.figure(figsize=(14, 8))
            
            sns.lineplot(
                data=results_df,
                x='top_n',
                y=metric,
                hue='model_name',
                style='feature_method',
                marker='o',
                markersize=8,
                palette='tab10'
            )
            
            plt.title(f'æ¨¡å‹æ€§èƒ½ ({metric}) vs. ç‰¹å¾æ•°é‡ (Top N)', fontsize=16)
            plt.xlabel('é€‰æ‹©çš„ç‰¹å¾æ•°é‡ (Top N)', fontsize=12)
            plt.ylabel(f'äº¤å‰éªŒè¯å¾—åˆ† ({metric})', fontsize=12)
            plt.grid(True, which='both', linestyle='--', linewidth=0.5)
            plt.legend(title='æ¨¡å‹/ç‰¹å¾é€‰æ‹©æ–¹æ³•', bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout(rect=[0, 0, 0.85, 1])

            if save_path:
                # ä¸ºæ¯ä¸ªæŒ‡æ ‡ä¿å­˜ä¸åŒçš„æ–‡ä»¶å
                metric_save_path = save_path.replace('.png', f'_{metric}.png')
                plt.savefig(metric_save_path, dpi=300, bbox_inches='tight')
                logger.info(f"Performance vs. top_n plot saved to: {metric_save_path}")

            plt.show()

    def evaluate_final_model(self, best_config: Dict, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> None:
        """
        å¯¹æœ€ç»ˆé€‰å®šçš„æœ€ä½³æ¨¡å‹é…ç½®è¿›è¡Œå…¨é¢çš„å¥å£®æ€§è¯„ä¼°ã€‚

        Args:
            best_config: åŒ…å«æœ€ä½³ top_n, feature_method, model_name çš„å­—å…¸ã€‚
            X_train, y_train: å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€‚
            X_test, y_test: ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®ã€‚
        """
        if not _PLOTTING_ENABLED: return

        print("\n" + "="*60)
        print("          FINAL MODEL ROBUSTNESS EVALUATION          ")
        print("="*60)
        
        # 1. åœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
        model_name = best_config['model_name']
        logger.info(f"Retraining the final best model: {model_name}...")
        model = self.get_models()[model_name]
        model.fit(X_train, y_train)
        
        # 2. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

        # 3. æ‰“å°æ ¸å¿ƒæŒ‡æ ‡
        print("\nğŸ“Š ç‹¬ç«‹æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆæ€§èƒ½:")
        print(classification_report(y_test, y_pred, target_names=['Not Readmitted', 'Readmitted']))

        # 4. ç»˜åˆ¶ä¸‰ç§æ ¸å¿ƒè¯„ä¼°å›¾
        fig, axes = plt.subplots(1, 3, figsize=(24, 7))
        fig.suptitle(f'å¯¹æœ€ä½³æ¨¡å‹ ({model_name}) çš„æœ€ç»ˆè¯„ä¼°', fontsize=18)

        self._plot_confusion_matrix(y_test, y_pred, ax=axes[0])
        self._plot_pr_curve(y_test, y_prob, ax=axes[1])
        self._plot_calibration_curve(y_test, y_prob, model_name, ax=axes[2])

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

    def _plot_confusion_matrix(self, y_true, y_pred, ax):
        """Helper to plot confusion matrix."""
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, 
                    xticklabels=['Predicted Not Readmitted', 'Predicted Readmitted'],
                    yticklabels=['Actual Not Readmitted', 'Actual Readmitted'])
        ax.set_title('æ··æ·†çŸ©é˜µ (Confusion Matrix)', fontsize=14)
        ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')
        ax.set_ylabel('çœŸå®æ ‡ç­¾')

    def _plot_pr_curve(self, y_true, y_prob, ax):
        """Helper to plot Precision-Recall curve."""
        from sklearn.metrics import precision_recall_curve, average_precision_score
        precision, recall, _ = precision_recall_curve(y_true, y_prob)
        ap_score = average_precision_score(y_true, y_prob)
        ax.plot(recall, precision, marker='.', label=f'AP = {ap_score:.3f}')
        ax.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ (PR Curve)', fontsize=14)
        ax.set_xlabel('å¬å›ç‡ (Recall)')
        ax.set_ylabel('ç²¾ç¡®ç‡ (Precision)')
        ax.grid(True, linestyle='--')
        ax.legend()

    def _plot_calibration_curve(self, y_true, y_prob, model_name, ax):
        """Helper to plot calibration curve."""
        from sklearn.calibration import calibration_curve
        prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10, strategy='uniform')
        ax.plot(prob_pred, prob_true, marker='o', label=model_name)
        ax.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')
        ax.set_title('æ ¡å‡†æ›²çº¿ (Calibration Curve)', fontsize=14)
        ax.set_xlabel('é¢„æµ‹æ¦‚ç‡çš„å¹³å‡å€¼ (Mean Predicted Probability)')
        ax.set_ylabel('æ­£ä¾‹çš„æ¯”ä¾‹ (Fraction of Positives)')
        ax.grid(True, linestyle='--')
        ax.legend()
    
    def evaluate_on_test_set(self, X_test: pd.DataFrame, y_test: pd.Series) -> pd.DataFrame:
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            X_test: æµ‹è¯•é›†ç‰¹å¾
            y_test: æµ‹è¯•é›†ç›®æ ‡å˜é‡
            
        Returns:
            æµ‹è¯•é›†è¯„ä¼°ç»“æœDataFrame
        """
        logger.info("Evaluating models on test set...")
        
        if not self.trained_models:
            logger.warning("No trained models available. Train models first.")
            return pd.DataFrame()
        
        test_results = []
        
        for model_name, model in self.trained_models.items():
            try:
                y_pred = model.predict(X_test)
                y_prob = model.predict_proba(X_test)[:, 1]
                
                results = {
                    'model_name': model_name,
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred),
                    'recall': recall_score(y_test, y_pred),
                    'f1': f1_score(y_test, y_pred),
                    'auc': roc_auc_score(y_test, y_prob)
                }
                
                test_results.append(results)
                self.test_results[model_name] = results
                
            except Exception as e:
                logger.error(f"Error evaluating {model_name} on test set: {e}")
        
        test_results_df = pd.DataFrame(test_results)
        logger.info("Test set evaluation completed")
        
        return test_results_df
    
    def get_best_model(self, metric: str = 'auc') -> Tuple[str, Any]:
        """
        è·å–æœ€ä½³æ¨¡å‹
        
        Args:
            metric: è¯„ä¼°æŒ‡æ ‡
            
        Returns:
            æœ€ä½³æ¨¡å‹åç§°å’Œæ¨¡å‹å¯¹è±¡
        """
        if not self.test_results:
            logger.warning("No test results available. Run evaluate_on_test_set first.")
            return None, None
        
        best_model_name = max(self.test_results.keys(), 
                            key=lambda x: self.test_results[x][metric])
        best_model = self.trained_models[best_model_name]
        
        logger.info(f"Best model by {metric}: {best_model_name} ({self.test_results[best_model_name][metric]:.3f})")
        return best_model_name, best_model
    
    def save_models(self, output_dir: str = 'outputs/models') -> None:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        if not self.trained_models:
            logger.warning("No trained models to save.")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        for model_name, model in self.trained_models.items():
            model_path = f"{output_dir}/{model_name}.joblib"
            joblib.dump(model, model_path)
            logger.info(f"Model {model_name} saved to {model_path}")
    
    def load_models(self, models_dir: str = 'outputs/models') -> None:
        """
        åŠ è½½ä¿å­˜çš„æ¨¡å‹
        
        Args:
            models_dir: æ¨¡å‹ç›®å½•
        """
        models_path = Path(models_dir)
        if not models_path.exists():
            logger.warning(f"Models directory {models_dir} does not exist.")
            return
        
        for model_file in models_path.glob("*.joblib"):
            model_name = model_file.stem
            model = joblib.load(model_file)
            self.trained_models[model_name] = model
            logger.info(f"Model {model_name} loaded from {model_file}")
    
    def generate_model_report(self, output_path: str = 'outputs/model_report.txt') -> None:
        """
        ç”Ÿæˆæ¨¡å‹è®­ç»ƒæŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        """
        if not self.test_results:
            logger.warning("No test results available for report generation.")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            f.write("Hospital Readmission Prediction - Model Training Report\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("Model Performance Summary:\n")
            f.write("-" * 30 + "\n")
            
            for model_name, results in self.test_results.items():
                f.write(f"\n{model_name}:\n")
                for metric, value in results.items():
                    if metric != 'model_name':
                        f.write(f"  {metric}: {value:.3f}\n")
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_auc_model = max(self.test_results.keys(), 
                               key=lambda x: self.test_results[x]['auc'])
            best_f1_model = max(self.test_results.keys(), 
                              key=lambda x: self.test_results[x]['f1'])
            
            f.write(f"\nBest Model by AUC: {best_auc_model} ({self.test_results[best_auc_model]['auc']:.3f})\n")
            f.write(f"Best Model by F1: {best_f1_model} ({self.test_results[best_f1_model]['f1']:.3f})\n")
        
        logger.info(f"Model report generated: {output_path}")
    
    def plot_model_comparison(self, save_path: str = 'outputs/model_comparison.png') -> None:
        """
        ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if not self.test_results:
            logger.warning("No test results available for plotting.")
            return
        
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']
        models = list(self.test_results.keys())
        
        fig, axes = plt.subplots(1, len(metrics), figsize=(15, 4))
        
        for i, metric in enumerate(metrics):
            values = [self.test_results[model][metric] for model in models]
            
            axes[i].bar(models, values)
            axes[i].set_title(f'{metric.upper()}')
            axes[i].set_ylabel('Score')
            axes[i].tick_params(axis='x', rotation=45)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, v in enumerate(values):
                axes[i].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Model comparison plot saved to: {save_path}")
        plt.show()

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹è®­ç»ƒåŠŸèƒ½"""
    from data_loader import DataLoader
    from data_preprocessor import DataPreprocessor
    from feature_selector import FeatureSelector
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    loader = DataLoader()
    df = loader.merge_data()
    
    preprocessor = DataPreprocessor()
    df = preprocessor.apply_feature_engineering(df)
    df = preprocessor.prepare_target_variable(df)
    
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(df)
    X_train, X_val, X_test = preprocessor.encode_categorical_features(X_train, X_val, X_test)
    X_train, X_val, X_test = preprocessor.scale_numerical_features(X_train, X_val, X_test)
    X_train_balanced, y_train_balanced = preprocessor.apply_smote(X_train, y_train)
    
    # åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨
    feature_selector = FeatureSelector()
    
    # ä½¿ç”¨å¤šä¸ªtop_nå€¼è¿è¡Œæ‰€æœ‰ç‰¹å¾é€‰æ‹©æ–¹æ³•
    multiple_feature_sets = feature_selector.select_features_multiple_topn(
        X_train_balanced, y_train_balanced, [5, 10, 15]
    )
    
    # æ˜¾ç¤ºç‰¹å¾é€‰æ‹©ç»“æœ
    feature_selector.display_multiple_topn_results(multiple_feature_sets)

    # åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
    model_trainer = ModelTrainer()

    # ä¸ºå¤šä¸ªç‰¹å¾é›†è®­ç»ƒæ¨¡å‹
    training_results = model_trainer.train_models_for_feature_sets(
        multiple_feature_sets,
        X_train_balanced,
        y_train_balanced
    )
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    model_trainer.display_training_results(training_results, metric='cv_f1')
    
    # å¯è§†åŒ–è®­ç»ƒç»“æœ
    model_trainer.plot_training_results(training_results, metric='cv_f1', save_path='outputs/training_performance_comparison.png')
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_results = model_trainer.evaluate_on_test_set(X_test, y_test)
    
    print("\nTest Results:")
    print(test_results)
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_model_name, best_model = model_trainer.get_best_model('auc')
    
    # ä¿å­˜æ¨¡å‹å’Œç”ŸæˆæŠ¥å‘Š
    model_trainer.save_models()
    model_trainer.generate_model_report()
    model_trainer.plot_model_comparison()
    
    # å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œå…¨é¢çš„å¥å£®æ€§è¯„ä¼°
    model_trainer.evaluate_final_model(best_config, X_train_balanced, y_train_balanced, X_test, y_test)
    
    return model_trainer, test_results

if __name__ == "__main__":
    main() 