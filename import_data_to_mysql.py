"""
æ•°æ®å¯¼å…¥è„šæœ¬ - å°†Azureæ•°æ®å¯¼å…¥MySQLæ•°æ®åº“

ä»Azureä¸‹è½½åŸå§‹æ•°æ®ï¼Œåº”ç”¨é¦–æ¬¡å…¥é™¢é€»è¾‘ï¼Œç„¶åå¯¼å…¥MySQL
"""

import pandas as pd
import mysql.connector
from mysql.connector import Error
import logging
import os
import tempfile

# æ·»åŠ Azureç›¸å…³å¯¼å…¥
try:
    from azure.storage.blob import BlobServiceClient
    from azure.identity import DefaultAzureCredential
    AZURE_AVAILABLE = True
except ImportError:
    AZURE_AVAILABLE = False
    print("âš ï¸ Azure SDK not available, will use local files")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataImporter:
    def __init__(self, host='localhost', database='hospital_readmission', 
                 user='root', password='hospital123'):
        self.host = host
        self.database = database
        self.user = user
        self.password = password
        self.connection = None
        
    def connect(self):
        """è¿æ¥åˆ°MySQLæ•°æ®åº“"""
        try:
            self.connection = mysql.connector.connect(
                host=self.host,
                database=self.database,
                user=self.user,
                password=self.password,
                charset='utf8mb4'
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.database}")
            return True
        except Error as e:
            logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.connection and self.connection.is_connected():
            self.connection.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def download_from_azure(self, container_name="raw-data", blob_name="diabetic_data.csv"):
        """
        ä»Azureä¸‹è½½åŸå§‹æ•°æ®æ–‡ä»¶
        
        Args:
            container_name: Azureå®¹å™¨åç§°
            blob_name: Azure blobåç§°
            
        Returns:
            str: ä¸‹è½½æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not AZURE_AVAILABLE:
            logger.warning("âš ï¸ Azure SDKä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            return None
            
        try:
            # å°è¯•ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
            
            if connection_string:
                blob_service_client = BlobServiceClient.from_connection_string(connection_string)
                logger.info("âœ… ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²è¿æ¥åˆ°Azure Storage")
            else:
                # ä½¿ç”¨é»˜è®¤å‡­æ®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
                blob_service_client = BlobServiceClient(
                    account_url="https://hospitalreadmissionstorage.blob.core.windows.net/",
                    credential=DefaultAzureCredential()
                )
                logger.info("âœ… ä½¿ç”¨é»˜è®¤å‡­æ®è¿æ¥åˆ°Azure Storage")
            
            # è·å–å®¹å™¨å’Œblobå®¢æˆ·ç«¯
            container_client = blob_service_client.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.csv')
            temp_file_path = temp_file.name
            temp_file.close()
            
            # ä¸‹è½½æ–‡ä»¶
            logger.info(f"ğŸ“¥ ä»Azureä¸‹è½½æ–‡ä»¶: {container_name}/{blob_name}")
            with open(temp_file_path, "wb") as download_file:
                download_stream = blob_client.download_blob()
                download_file.write(download_stream.readall())
            
            logger.info(f"âœ… æ–‡ä»¶å·²ä¸‹è½½åˆ°: {temp_file_path}")
            return temp_file_path
            
        except Exception as e:
            logger.error(f"âŒ ä»Azureä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            logger.info("ğŸ’¡ å°†å°è¯•ä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            return None
    
    def clear_tables(self):
        """æ¸…ç©ºæ‰€æœ‰è¡¨ï¼ˆæŒ‰æ­£ç¡®é¡ºåºå¤„ç†å¤–é”®çº¦æŸï¼‰"""
        try:
            cursor = self.connection.cursor()
            
            # æŒ‰é¡ºåºåˆ é™¤è¡¨ï¼ˆå…ˆåˆ é™¤æœ‰å¤–é”®å¼•ç”¨çš„è¡¨ï¼‰
            tables_to_drop = [
                'model_results',  # æ²¡æœ‰å¤–é”®å¼•ç”¨
                'medications',    # å¯èƒ½å¼•ç”¨encounters
                'encounters',     # å¼•ç”¨patients
                'patients'        # è¢«å…¶ä»–è¡¨å¼•ç”¨
            ]
            
            for table in tables_to_drop:
                cursor.execute(f"DROP TABLE IF EXISTS {table}")
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¡¨: {table}")
            
            self.connection.commit()
            cursor.close()
            logger.info("âœ… æ‰€æœ‰è¡¨å·²æ¸…ç©º")
            return True
            
        except Error as e:
            logger.error(f"âŒ æ¸…ç©ºè¡¨å¤±è´¥: {e}")
            return False
    
    def recreate_tables(self):
        """é‡æ–°åˆ›å»ºè¡¨ç»“æ„"""
        try:
            cursor = self.connection.cursor()
            
            # åˆ›å»ºpatientsè¡¨ - åŒ…å«æ‰€æœ‰50åˆ—åŸå§‹æ•°æ®
            patients_table = """
            CREATE TABLE patients (
                id INT AUTO_INCREMENT PRIMARY KEY,
                encounter_id VARCHAR(50),
                patient_id VARCHAR(50) UNIQUE NOT NULL,
                race VARCHAR(50),
                gender VARCHAR(20),
                age INT,
                weight VARCHAR(20),
                admission_type_id INT,
                discharge_disposition_id INT,
                admission_source_id INT,
                time_in_hospital INT,
                payer_code VARCHAR(20),
                medical_specialty VARCHAR(100),
                num_lab_procedures INT,
                num_procedures INT,
                num_medications INT,
                number_outpatient INT,
                number_emergency INT,
                number_inpatient INT,
                diag_1 VARCHAR(10),
                diag_2 VARCHAR(10),
                diag_3 VARCHAR(10),
                number_diagnoses INT,
                max_glu_serum VARCHAR(20),
                A1Cresult VARCHAR(20),
                metformin VARCHAR(10),
                repaglinide VARCHAR(10),
                nateglinide VARCHAR(10),
                chlorpropamide VARCHAR(10),
                glimepiride VARCHAR(10),
                acetohexamide VARCHAR(10),
                glipizide VARCHAR(10),
                glyburide VARCHAR(10),
                tolbutamide VARCHAR(10),
                pioglitazone VARCHAR(10),
                rosiglitazone VARCHAR(10),
                acarbose VARCHAR(10),
                miglitol VARCHAR(10),
                troglitazone VARCHAR(10),
                tolazamide VARCHAR(10),
                examide VARCHAR(10),
                citoglipton VARCHAR(10),
                insulin VARCHAR(10),
                `glyburide-metformin` VARCHAR(10),
                `glipizide-metformin` VARCHAR(10),
                `glimepiride-pioglitazone` VARCHAR(10),
                `metformin-rosiglitazone` VARCHAR(10),
                `metformin-pioglitazone` VARCHAR(10),
                medication_change VARCHAR(10),
                diabetesMed VARCHAR(10),
                readmitted VARCHAR(10),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_patient_id (patient_id),
                INDEX idx_encounter_id (encounter_id),
                INDEX idx_readmitted (readmitted),
                INDEX idx_age (age),
                INDEX idx_time_in_hospital (time_in_hospital),
                INDEX idx_number_diagnoses (number_diagnoses),
                INDEX idx_medical_specialty (medical_specialty)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            
            # åˆ›å»ºencountersè¡¨
            encounters_table = """
            CREATE TABLE encounters (
                id INT AUTO_INCREMENT PRIMARY KEY,
                encounter_id VARCHAR(50) UNIQUE NOT NULL,
                patient_id VARCHAR(50),
                encounter_date DATE,
                discharge_date DATE,
                length_of_stay INT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (patient_id) REFERENCES patients(patient_id) ON DELETE CASCADE,
                INDEX idx_encounter_id (encounter_id),
                INDEX idx_patient_id (patient_id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            
            # åˆ›å»ºmedicationsè¡¨
            medications_table = """
            CREATE TABLE medications (
                id INT AUTO_INCREMENT PRIMARY KEY,
                encounter_id VARCHAR(50),
                medication_name VARCHAR(100),
                dosage VARCHAR(50),
                frequency VARCHAR(50),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (encounter_id) REFERENCES encounters(encounter_id) ON DELETE CASCADE,
                INDEX idx_encounter_id (encounter_id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            
            # åˆ›å»ºmodel_resultsè¡¨
            model_results_table = """
            CREATE TABLE model_results (
                id INT AUTO_INCREMENT PRIMARY KEY,
                model_name VARCHAR(50),
                feature_selection_method VARCHAR(50),
                accuracy DECIMAL(5,4),
                precision_score DECIMAL(5,4),
                recall DECIMAL(5,4),
                f1_score DECIMAL(5,4),
                auc_score DECIMAL(5,4),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_model_name (model_name),
                INDEX idx_created_at (created_at)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            
            # æ‰§è¡Œåˆ›å»ºè¡¨
            tables = [
                ("patients", patients_table),
                ("encounters", encounters_table),
                ("medications", medications_table),
                ("model_results", model_results_table)
            ]
            
            for table_name, table_sql in tables:
                cursor.execute(table_sql)
                logger.info(f"âœ… åˆ›å»ºè¡¨: {table_name}")
            
            self.connection.commit()
            cursor.close()
            logger.info("âœ… æ‰€æœ‰è¡¨ç»“æ„é‡æ–°åˆ›å»ºå®Œæˆ")
            return True
            
        except Error as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            return False
    
    def import_csv_data(self, csv_file='diabetic_data.csv', sample_size=None, use_azure=True):
        """
        å¯¼å…¥æ•°æ®åˆ°patientsè¡¨ï¼Œä¸¥æ ¼æŒ‰ç…§é¦–æ¬¡å…¥é™¢ä¸šåŠ¡é€»è¾‘
        
        Args:
            csv_file: æœ¬åœ°CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
            sample_size: é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨æ•°æ®ï¼‰
            use_azure: æ˜¯å¦ä¼˜å…ˆä»Azureä¸‹è½½æ•°æ®
        """
        try:
            file_to_use = csv_file
            
            # ä¼˜å…ˆä»Azureä¸‹è½½æ•°æ®
            if use_azure:
                logger.info("â˜ï¸ å°è¯•ä»Azureä¸‹è½½åŸå§‹æ•°æ®...")
                azure_file = self.download_from_azure()
                if azure_file:
                    file_to_use = azure_file
                    logger.info(f"âœ… ä½¿ç”¨Azureæ•°æ®: {azure_file}")
                else:
                    logger.info(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: {csv_file}")
            
            logger.info(f"ğŸ“ å¼€å§‹å¯¼å…¥æ•°æ®æ–‡ä»¶: {file_to_use}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(file_to_use)
            logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            
            # åªä¿ç•™é¦–æ¬¡å…¥é™¢è®°å½•ï¼ˆnotebooké€»è¾‘ï¼‰
            df_sorted = df.sort_values(by='encounter_id')
            df_first = df_sorted.drop_duplicates(subset='patient_nbr', keep='first')
            logger.info(f"ğŸ“Š é¦–æ¬¡å…¥é™¢ç­›é€‰å: {len(df_first)} è¡Œ")
            
            # ä¸å†é‡‡æ ·ï¼Œå…¨éƒ¨å¯¼å…¥
            # if sample_size is not None and len(df_first) > sample_size:
            #     df_first = df_first.sample(n=sample_size, random_state=42)
            #     logger.info(f"ğŸ“Š ä½¿ç”¨æ ·æœ¬æ•°æ®: {len(df_first)} è¡Œ")
            
            # æ•°æ®æ¸…æ´—
            df_first = self.clean_data(df_first)
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            cursor = self.connection.cursor()
            
            # æ„å»ºINSERTè¯­å¥ - åŒ…å«æ‰€æœ‰50åˆ—åŸå§‹æ•°æ®
            columns = [
                'encounter_id', 'patient_id', 'race', 'gender', 'age', 'weight',
                'admission_type_id', 'discharge_disposition_id', 'admission_source_id',
                'time_in_hospital', 'payer_code', 'medical_specialty',
                'num_lab_procedures', 'num_procedures', 'num_medications',
                'number_outpatient', 'number_emergency', 'number_inpatient',
                'diag_1', 'diag_2', 'diag_3', 'number_diagnoses',
                'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',
                'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',
                'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone',
                'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide',
                'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin',
                'glimepiride-pioglitazone', 'metformin-rosiglitazone',
                'metformin-pioglitazone', 'medication_change', 'diabetesMed', 'readmitted'
            ]
            
            # é‡å‘½ååˆ—åï¼ˆå¦‚æœéœ€è¦ï¼‰
            column_mapping = {
                'patient_nbr': 'patient_id',
                'change': 'medication_change'
            }
            df_first = df_first.rename(columns=column_mapping)
            
            # æ’å…¥æ•°æ® - å¤„ç†åˆ—åä¸­çš„è¿å­—ç¬¦
            columns_quoted = [f"`{col}`" for col in columns]
            insert_query = f"""
            INSERT INTO patients ({', '.join(columns_quoted)})
            VALUES ({', '.join(['%s'] * len(columns))})
            """
            
            # å‡†å¤‡æ•°æ®
            data_to_insert = []
            for _, row in df_first.iterrows():
                row_data = []
                for col in columns:
                    value = row.get(col, None)
                    # å¤„ç†ç‰¹æ®Šå€¼
                    if pd.isna(value) or value == '?':
                        value = None
                    row_data.append(value)
                data_to_insert.append(row_data)
            
            # æ‰¹é‡æ’å…¥
            cursor.executemany(insert_query, data_to_insert)
            self.connection.commit()
            
            inserted_count = cursor.rowcount
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} è¡Œæ•°æ®åˆ°patientsè¡¨")
            
            cursor.close()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if use_azure and file_to_use != csv_file and os.path.exists(file_to_use):
                try:
                    os.unlink(file_to_use)
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_to_use}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            return True
            
        except Error as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ•°æ®å¤±è´¥: {e}")
            return False
    
    def clean_data(self, df):
        """ä¼ä¸šçº§æ•°æ®æ¸…æ´— - å¤„ç†æ‰€æœ‰50åˆ—"""
        logger.info("ğŸ§¹ å¼€å§‹ä¼ä¸šçº§æ•°æ®æ¸…æ´—...")
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        df = df.fillna('Unknown')
        logger.info("âœ… å¤„ç†ç¼ºå¤±å€¼å®Œæˆ")
        
        # 2. å¤„ç†ç‰¹æ®Šå­—ç¬¦
        df = df.replace('?', 'Unknown')
        logger.info("âœ… å¤„ç†ç‰¹æ®Šå­—ç¬¦å®Œæˆ")
        
        # 3. ç¡®ä¿patient_idæ˜¯å­—ç¬¦ä¸²
        if 'patient_id' in df.columns:
            df['patient_id'] = df['patient_id'].astype(str)
        
        # 4. å¤„ç†å¹´é¾„å­—æ®µ - æå–å¹´é¾„èŒƒå›´çš„ä¸­ç‚¹
        if 'age' in df.columns:
            def extract_age_midpoint(age_str):
                if pd.isna(age_str) or age_str == 'Unknown':
                    return None
                try:
                    # å¤„ç†æ ¼å¼å¦‚ '[70-80)' æˆ– '[0-10)' ç­‰
                    if isinstance(age_str, str) and '[' in age_str and ')' in age_str:
                        # æå–æ•°å­—èŒƒå›´
                        age_range = age_str.replace('[', '').replace(')', '')
                        if '-' in age_range:
                            start, end = age_range.split('-')
                            return int((int(start) + int(end)) / 2)
                        else:
                            return int(age_range)
                    else:
                        return int(age_str)
                except:
                    return None
            
            df['age'] = df['age'].apply(extract_age_midpoint)
            logger.info("âœ… å¹´é¾„å­—æ®µå¤„ç†å®Œæˆ")
        
        # 5. å¤„ç†æ•°å€¼å­—æ®µ - ç¡®ä¿ç±»å‹æ­£ç¡®
        numeric_columns = [
            'admission_type_id', 'discharge_disposition_id', 'admission_source_id',
            'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications',
            'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses'
        ]
        
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col] = df[col].fillna(0).astype(int)
        
        logger.info("âœ… æ•°å€¼å­—æ®µå¤„ç†å®Œæˆ")
        
        # 6. å¤„ç†è¯ç‰©å­—æ®µ - æ ‡å‡†åŒ–
        medication_columns = [
            'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',
            'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',
            'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',
            'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin',
            'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone'
        ]
        
        for col in medication_columns:
            if col in df.columns:
                df[col] = df[col].str.upper()
                df[col] = df[col].replace(['UNKNOWN', 'NONE'], 'No')
        
        logger.info("âœ… è¯ç‰©å­—æ®µæ ‡å‡†åŒ–å®Œæˆ")
        
        # 7. å¤„ç†è¯Šæ–­å­—æ®µ - æ¸…ç†æ ¼å¼
        diagnosis_columns = ['diag_1', 'diag_2', 'diag_3']
        for col in diagnosis_columns:
            if col in df.columns:
                df[col] = df[col].str.strip()
                df[col] = df[col].replace('', 'Unknown')
        
        logger.info("âœ… è¯Šæ–­å­—æ®µå¤„ç†å®Œæˆ")
        
        # 8. æ•°æ®è´¨é‡æ£€æŸ¥
        total_rows = len(df)
        null_counts = df.isnull().sum()
        logger.info(f"ğŸ“Š æ•°æ®æ¸…æ´—å®Œæˆ - æ€»è¡Œæ•°: {total_rows}")
        logger.info(f"ğŸ“Š å„åˆ—ç©ºå€¼ç»Ÿè®¡: {null_counts.sum()} ä¸ªç©ºå€¼")
        
        return df
    
    def show_table_info(self):
        """æ˜¾ç¤ºè¡¨ä¿¡æ¯"""
        try:
            cursor = self.connection.cursor()
            
            tables = ['patients', 'encounters', 'medications', 'model_results']
            
            for table in tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                logger.info(f"ğŸ“Š {table}: {count} è¡Œ")
            
            cursor.close()
            
        except Error as e:
            logger.error(f"âŒ è·å–è¡¨ä¿¡æ¯å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 60)
    
    # åˆ›å»ºå¯¼å…¥å™¨
    importer = DataImporter()
    
    # è¿æ¥æ•°æ®åº“
    if not importer.connect():
        return
    
    try:
        # æ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºè¡¨
        print("\nğŸ“‹ æ­¥éª¤1: æ¸…ç©ºç°æœ‰è¡¨")
        if not importer.clear_tables():
            return
        
        print("\nğŸ“‹ æ­¥éª¤2: é‡æ–°åˆ›å»ºè¡¨ç»“æ„")
        if not importer.recreate_tables():
            return
        
        # å¯¼å…¥æ•°æ®
        print("\nğŸ“‹ æ­¥éª¤3: å¯¼å…¥CSVæ•°æ®")
        if not importer.import_csv_data():
            return
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“‹ æ­¥éª¤4: æ˜¾ç¤ºå¯¼å…¥ç»“æœ")
        importer.show_table_info()
        
        print("\nğŸ‰ æ•°æ®å¯¼å…¥å®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨ä½ å¯ä»¥åœ¨Navicatä¸­æŸ¥çœ‹æ•°æ®äº†")
        
    finally:
        importer.disconnect()

if __name__ == "__main__":
    main() 