#!/usr/bin/env python3
"""
ä¸Šä¼ æ¸…æ´—åçš„æ•°æ®è¡¨åˆ°Azure Data Lake
å°†patients_cleanedå’Œpatients_business_cleanedä¸¤ä¸ªè¡¨ä¸Šä¼ åˆ°processed-dataå®¹å™¨
"""
import pandas as pd
import mysql.connector
from mysql.connector import Error
import logging
from datetime import datetime
import os
from azure.storage.blob import BlobServiceClient
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class CleanedTablesUploader:
    def __init__(self, host='localhost', database='hospital_readmission', 
                 user='root', password='hospital123'):
        self.host = host
        self.database = database
        self.user = user
        self.password = password
        self.connection = None
        
    def connect(self):
        """è¿æ¥åˆ°MySQLæ•°æ®åº“"""
        try:
            self.connection = mysql.connector.connect(
                host=self.host,
                database=self.database,
                user=self.user,
                password=self.password
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.database}")
            return True
        except Error as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.connection and self.connection.is_connected():
            self.connection.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def get_cleaned_data(self, table_name):
        """è·å–æ¸…æ´—åçš„æ•°æ®"""
        query = f"SELECT * FROM {table_name}"
        df = pd.read_sql(query, self.connection)
        logger.info(f"ğŸ“Š è·å–{table_name}æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        return df
    
    def save_to_csv(self, df, filename):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            df.to_csv(filename, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
            return False
    
    def upload_to_azure(self, local_file, container_name="processed-data", blob_name=None):
        """ä¸Šä¼ æ–‡ä»¶åˆ°Azure"""
        try:
            # è·å–Azureè¿æ¥å­—ç¬¦ä¸²
            connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
            if not connection_string:
                logger.error("âŒ æœªæ‰¾åˆ°AZURE_STORAGE_CONNECTION_STRINGç¯å¢ƒå˜é‡")
                return False
            
            # åˆ›å»ºBlobæœåŠ¡å®¢æˆ·ç«¯
            blob_service_client = BlobServiceClient.from_connection_string(connection_string)
            
            # è·å–å®¹å™¨å®¢æˆ·ç«¯
            container_client = blob_service_client.get_container_client(container_name)
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šblobåç§°ï¼Œä½¿ç”¨æ–‡ä»¶å
            if not blob_name:
                blob_name = os.path.basename(local_file)
            
            # è·å–blobå®¢æˆ·ç«¯
            blob_client = container_client.get_blob_client(blob_name)
            
            # ä¸Šä¼ æ–‡ä»¶
            with open(local_file, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)
            
            logger.info(f"âœ… æˆåŠŸä¸Šä¼ åˆ°Azure: {container_name}/{blob_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Azureä¸Šä¼ å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ¸…æ´—è¡¨ä¸Šä¼ å·¥å…·")
    print("=" * 60)
    
    uploader = CleanedTablesUploader()
    if not uploader.connect():
        return
    
    try:
        # ä¸Šä¼ ç¬¬ä¸€ä¸ªè¡¨ï¼špatients_cleaned
        print("\nğŸ“Š ä¸Šä¼  patients_cleaned è¡¨...")
        df_cleaned = uploader.get_cleaned_data('patients_cleaned')
        if df_cleaned is None:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        local_file_cleaned = f"patients_cleaned_{timestamp}.csv"
        
        if uploader.save_to_csv(df_cleaned, local_file_cleaned):
            if uploader.upload_to_azure(local_file_cleaned, blob_name="patients_cleaned.csv"):
                print("âœ… patients_cleaned ä¸Šä¼ æˆåŠŸï¼")
                print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df_cleaned)} è¡Œ, {len(df_cleaned.columns)} åˆ—")
                print(f"â˜ï¸ Azureä½ç½®: processed-data/patients_cleaned.csv")
            else:
                print("âŒ patients_cleaned Azureä¸Šä¼ å¤±è´¥")
        
        # ä¸Šä¼ ç¬¬äºŒä¸ªè¡¨ï¼špatients_business_cleaned
        print("\nğŸ“Š ä¸Šä¼  patients_business_cleaned è¡¨...")
        df_business = uploader.get_cleaned_data('patients_business_cleaned')
        if df_business is None:
            return
        
        local_file_business = f"patients_business_cleaned_{timestamp}.csv"
        
        if uploader.save_to_csv(df_business, local_file_business):
            if uploader.upload_to_azure(local_file_business, blob_name="patients_business_cleaned.csv"):
                print("âœ… patients_business_cleaned ä¸Šä¼ æˆåŠŸï¼")
                print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df_business)} è¡Œ, {len(df_business.columns)} åˆ—")
                print(f"â˜ï¸ Azureä½ç½®: processed-data/patients_business_cleaned.csv")
            else:
                print("âŒ patients_business_cleaned Azureä¸Šä¼ å¤±è´¥")
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        try:
            os.remove(local_file_cleaned)
            os.remove(local_file_business)
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†æœ¬åœ°æ–‡ä»¶: {local_file_cleaned}, {local_file_business}")
        except:
            pass
        
        print("\nğŸ‰ æ‰€æœ‰æ¸…æ´—è¡¨ä¸Šä¼ å®Œæˆï¼")
        print("=" * 60)
        print("ğŸ“‹ ä¸Šä¼ æ€»ç»“:")
        print(f"   - patients_cleaned: {len(df_cleaned):,} è¡Œ, {len(df_cleaned.columns)} åˆ—")
        print(f"   - patients_business_cleaned: {len(df_business):,} è¡Œ, {len(df_business.columns)} åˆ—")
        print(f"   - æ€»è®°å½•æ•°: {len(df_cleaned) + len(df_business):,} è¡Œ")
            
    finally:
        uploader.disconnect()

if __name__ == "__main__":
    main() 